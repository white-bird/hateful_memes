{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "import cv2\n",
    "import traceback\n",
    "from PIL import Image\n",
    "from torchvision import transforms as T\n",
    "# nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "data = []\n",
    "f = open('../input2/data/train.jsonl','r')\n",
    "for line in f.readlines():\n",
    "    jsonobject = json.loads(line)\n",
    "    nltksen = sid.polarity_scores(jsonobject['text'])\n",
    "    data.append([jsonobject['id'],jsonobject['text'],jsonobject['img'],jsonobject['label'],\n",
    "                nltksen['neg'],nltksen['neu'],nltksen['pos'],nltksen['compound']])\n",
    "    \n",
    "df = pd.DataFrame(data,columns = ['id','text','img',\n",
    "                                 'label','nltk1','nltk2','nltk3','nltk4'])\n",
    "df.to_csv('../input2/train.csv',index = None)\n",
    "\n",
    "data = []\n",
    "f = open('../input2/data/dev_seen.jsonl','r')\n",
    "for line in f.readlines():\n",
    "    jsonobject = json.loads(line)\n",
    "    nltksen = sid.polarity_scores(jsonobject['text'])\n",
    "    data.append([jsonobject['id'],jsonobject['text'],jsonobject['img'],jsonobject['label'],\n",
    "                nltksen['neg'],nltksen['neu'],nltksen['pos'],nltksen['compound']])\n",
    "    \n",
    "df = pd.DataFrame(data,columns = ['id','text','img',\n",
    "                                 'label','nltk1','nltk2','nltk3','nltk4'])\n",
    "df.to_csv('../input2/dev1.csv',index = None)\n",
    "\n",
    "data = []\n",
    "f = open('../input2/data/dev_unseen.jsonl','r')\n",
    "for line in f.readlines():\n",
    "    jsonobject = json.loads(line)\n",
    "    nltksen = sid.polarity_scores(jsonobject['text'])\n",
    "    data.append([jsonobject['id'],jsonobject['text'],jsonobject['img'],jsonobject['label'],\n",
    "                nltksen['neg'],nltksen['neu'],nltksen['pos'],nltksen['compound']])\n",
    "    \n",
    "df = pd.DataFrame(data,columns = ['id','text','img',\n",
    "                                 'label','nltk1','nltk2','nltk3','nltk4'])\n",
    "df.to_csv('../input2/dev2.csv',index = None)\n",
    "\n",
    "data = []\n",
    "f = open('../input2/data/test_seen.jsonl','r')\n",
    "for line in f.readlines():\n",
    "    jsonobject = json.loads(line)\n",
    "    nltksen = sid.polarity_scores(jsonobject['text'])\n",
    "    data.append([jsonobject['id'],jsonobject['text'],jsonobject['img'],1,\n",
    "                nltksen['neg'],nltksen['neu'],nltksen['pos'],nltksen['compound']])\n",
    "    \n",
    "df = pd.DataFrame(data,columns = ['id','text','img',\n",
    "                                 'label','nltk1','nltk2','nltk3','nltk4'])\n",
    "df.to_csv('../input2/test1.csv',index = None)\n",
    "\n",
    "data = []\n",
    "f = open('../input2/data/test_unseen.jsonl','r')\n",
    "for line in f.readlines():\n",
    "    jsonobject = json.loads(line)\n",
    "    nltksen = sid.polarity_scores(jsonobject['text'])\n",
    "    data.append([jsonobject['id'],jsonobject['text'],jsonobject['img'],1,\n",
    "                nltksen['neg'],nltksen['neu'],nltksen['pos'],nltksen['compound']])\n",
    "    \n",
    "df = pd.DataFrame(data,columns = ['id','text','img',\n",
    "                                 'label','nltk1','nltk2','nltk3','nltk4'])\n",
    "df.to_csv('../input2/test2.csv',index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# data = []\n",
    "# f = open('../input/data/train.jsonl','r')\n",
    "# for line in f.readlines():\n",
    "#     jsonobject = json.loads(line)\n",
    "#     nltksen = sid.polarity_scores(jsonobject['text'])\n",
    "#     data.append([jsonobject['id'],jsonobject['text'],jsonobject['img'],jsonobject['label'],\n",
    "#                 nltksen['neg'],nltksen['neu'],nltksen['pos'],nltksen['compound']])\n",
    "    \n",
    "# df = pd.DataFrame(data,columns = ['id','text','img',\n",
    "#                                  'label','nltk1','nltk2','nltk3','nltk4'])\n",
    "# df.to_csv('../input2/train_p1.csv',index = None)\n",
    "\n",
    "# data = []\n",
    "# f = open('../input/data/dev.jsonl','r')\n",
    "# for line in f.readlines():\n",
    "#     jsonobject = json.loads(line)\n",
    "#     nltksen = sid.polarity_scores(jsonobject['text'])\n",
    "#     data.append([jsonobject['id'],jsonobject['text'],jsonobject['img'],jsonobject['label'],\n",
    "#                 nltksen['neg'],nltksen['neu'],nltksen['pos'],nltksen['compound']])\n",
    "    \n",
    "# df = pd.DataFrame(data,columns = ['id','text','img',\n",
    "#                                  'label','nltk1','nltk2','nltk3','nltk4'])\n",
    "# df.to_csv('../input2/dev_p1.csv',index = None)\n",
    "\n",
    "# data = []\n",
    "# f = open('../input/data/test.jsonl','r')\n",
    "# for line in f.readlines():\n",
    "#     jsonobject = json.loads(line)\n",
    "#     nltksen = sid.polarity_scores(jsonobject['text'])\n",
    "#     data.append([jsonobject['id'],jsonobject['text'],jsonobject['img'],1,\n",
    "#                 nltksen['neg'],nltksen['neu'],nltksen['pos'],nltksen['compound']])\n",
    "    \n",
    "# df = pd.DataFrame(data,columns = ['id','text','img',\n",
    "#                                  'label','nltk1','nltk2','nltk3','nltk4'])\n",
    "# df.to_csv('../input2/test1_p1.csv',index = None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 256\n",
    "def resize_to_square(im):\n",
    "    old_size = im.shape[:2] # old_size is in (height, width) format\n",
    "    ratio = float(img_size)/max(old_size)\n",
    "    new_size = tuple([int(x*ratio) for x in old_size])\n",
    "    # new_size should be in (width, height) format\n",
    "    im = cv2.resize(im, (new_size[1], new_size[0]))\n",
    "    delta_w = img_size - new_size[1]\n",
    "    delta_h = img_size - new_size[0]\n",
    "    top, bottom = delta_h//2, delta_h-(delta_h//2)\n",
    "    left, right = delta_w//2, delta_w-(delta_w//2)\n",
    "    color = [0, 0, 0]\n",
    "    new_im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT,value=color)\n",
    "    return new_im\n",
    "\n",
    "def load_image(path, name):\n",
    "    image = (Image.open(path + name))\n",
    "    \n",
    "    transform1 = T.Compose([\n",
    "        T.Scale(img_size),\n",
    "        T.CenterCrop((img_size, img_size)),\n",
    "    ])\n",
    "    new_image = transform1(image)\n",
    "    new_image = np.array(new_image)\n",
    "    if len(new_image.shape) == 2:\n",
    "        new_image = np.repeat(new_image.reshape(img_size,img_size,1),3,axis = 2)\n",
    "    \n",
    "    if new_image.shape[2] > 3:\n",
    "        new_image = new_image[:,:,:3]\n",
    "    return np.array(new_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input2/train.csv')\n",
    "train2 = pd.read_csv('../input2/dev1.csv')\n",
    "test1 = pd.read_csv('../input2/test1.csv')\n",
    "train3 = pd.read_csv('../input2/dev2.csv')\n",
    "test2 = pd.read_csv('../input2/test2.csv')\n",
    "test = test1.append(test2)\n",
    "train2 = train3.append(train2).drop_duplicates('id',keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "6814\n",
      "13628\n"
     ]
    }
   ],
   "source": [
    "id_pic = {}\n",
    "id_label = {}\n",
    "cache = train[['id','text','label']].values\n",
    "for i in range(cache.shape[0]):\n",
    "    id_pic[cache[i,0]]  = cache[i,1]\n",
    "    id_label[cache[i,0]]  = cache[i,2]\n",
    "    \n",
    "cache = train2[['id','text','label']].values\n",
    "for i in range(cache.shape[0]):\n",
    "    id_pic[cache[i,0]]  = cache[i,1]\n",
    "    id_label[cache[i,0]]  = cache[i,2]\n",
    "    \n",
    "cache = test[['id','text']].values\n",
    "for i in range(cache.shape[0]):\n",
    "    id_pic[cache[i,0]]  = cache[i,1]\n",
    "    \n",
    "ids_list = list(id_pic.keys())  \n",
    "isfound = set()\n",
    "features2 = {}\n",
    "features = {}\n",
    "pairs = 0\n",
    "count1 = 0\n",
    "count2 = 0\n",
    "f = open('./text_pairs.csv','w+')\n",
    "for id,v in id_pic.items():\n",
    "    key = v.replace('\"','').replace(\"'\",'').replace(\" \",'')\n",
    "    features[id] = key\n",
    "    for id2,v in features.items():\n",
    "        if id == id2 or str(id) + ' ' + str(id2) in isfound:\n",
    "            continue\n",
    "        if v == key:\n",
    "            print(str(id) + ' ' + str(id2),file = f)\n",
    "            pairs += 1\n",
    "            isfound.add(str(id) + ' ' + str(id2))\n",
    "            isfound.add(str(id2) + ' ' + str(id))\n",
    "        \n",
    "print(count1,count2)\n",
    "print(pairs)\n",
    "print(len(isfound))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yingzhenzhe/.local/lib/python3.6/site-packages/torchvision/transforms/transforms.py:257: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
      "  \"please use transforms.Resize instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3230\n",
      "4807\n",
      "6151\n",
      "6937\n",
      "13628\n"
     ]
    }
   ],
   "source": [
    "id_pic = {}\n",
    "id_label = {}\n",
    "cache = train[['id','img','label']].values\n",
    "for i in range(cache.shape[0]):\n",
    "    id_pic[cache[i,0]]  = cache[i,1]\n",
    "    id_label[cache[i,0]]  = cache[i,2]\n",
    "    \n",
    "cache = train2[['id','img','label']].values\n",
    "for i in range(cache.shape[0]):\n",
    "    id_pic[cache[i,0]]  = cache[i,1]\n",
    "    id_label[cache[i,0]]  = cache[i,2]\n",
    "    \n",
    "cache = test[['id','img']].values\n",
    "for i in range(cache.shape[0]):\n",
    "    id_pic[cache[i,0]]  = cache[i,1]\n",
    "ids_list = list(id_pic.keys())        \n",
    "batch_size = 16\n",
    "features = {}\n",
    "features2 = {}\n",
    "n_batches = len(ids_list)//batch_size + 1\n",
    "pairs = 0\n",
    "f = open('./img_pairs.csv','w+')\n",
    "batch_ids = ids_list\n",
    "\n",
    "isfound3 = set()\n",
    "for i,id in enumerate(batch_ids):\n",
    "    if len(batch_ids) == 0:\n",
    "        continue\n",
    "    try:\n",
    "        image = load_image(\"../input2/data/\", id_pic[id])\n",
    "        features[id] = image[160:188:2,160:188:2,:].astype(int)\n",
    "        if len(set(features[id].sum(axis = 2).reshape(-1).tolist())) <= 1:\n",
    "            continue\n",
    "        for id2,v in features.items():\n",
    "            if id == id2 or str(id) + ' ' + str(id2) in isfound3:\n",
    "                continue\n",
    "            if abs(np.mean(v - features[id])) < 2.5 and np.std(v - features[id]) < 2.5 and np.std(features[id].sum(axis = 2)) > 5:\n",
    "                print(str(id) + ' ' + str(id2),file = f)\n",
    "                isfound3.add(str(id) + ' ' + str(id2))\n",
    "                isfound3.add(str(id2) + ' ' + str(id))\n",
    "                pairs += 1\n",
    "                break\n",
    "                    \n",
    "    except:\n",
    "        print(id,str(traceback.format_exc()))\n",
    "        \n",
    "features = {}\n",
    "print(pairs)\n",
    "for i,id in enumerate(batch_ids):\n",
    "    if len(batch_ids) == 0:\n",
    "        continue\n",
    "    try:\n",
    "        image = load_image(\"../input2/data/\", id_pic[id])\n",
    "        features[id] = image[175:190:1,150:165:1,:].astype(int)\n",
    "        if len(set(features[id].sum(axis = 2).reshape(-1).tolist())) <= 1:\n",
    "            continue\n",
    "        for id2,v in features.items():\n",
    "            if id == id2 or str(id) + ' ' + str(id2) in isfound3:\n",
    "                continue\n",
    "            if abs(np.mean(v - features[id])) < 2.5 and np.std(v - features[id]) < 2.5 and np.std(features[id].sum(axis = 2)) > 5:\n",
    "                print(str(id) + ' ' + str(id2),file = f)\n",
    "                isfound3.add(str(id) + ' ' + str(id2))\n",
    "                isfound3.add(str(id2) + ' ' + str(id))\n",
    "                pairs += 1\n",
    "                break\n",
    "                    \n",
    "    except:\n",
    "        print(id,str(traceback.format_exc()))        \n",
    "\n",
    "features = {}\n",
    "print(pairs)\n",
    "for i,id in enumerate(batch_ids):\n",
    "    if len(batch_ids) == 0:\n",
    "        continue\n",
    "    try:\n",
    "        image = load_image(\"../input2/data/\", id_pic[id])\n",
    "        features[id] = image[100:128:2,100:128:2,:].astype(int)\n",
    "        if len(set(features[id].sum(axis = 2).reshape(-1).tolist())) <= 1:\n",
    "            continue\n",
    "        for id2,v in features.items():\n",
    "            if id == id2 or str(id) + ' ' + str(id2) in isfound3:\n",
    "                continue\n",
    "            if abs(np.mean(v - features[id])) < 2.5 and np.std(v - features[id]) < 2.5 and np.std(features[id].sum(axis = 2)) > 5:\n",
    "                print(str(id) + ' ' + str(id2),file = f)\n",
    "                isfound3.add(str(id) + ' ' + str(id2))\n",
    "                isfound3.add(str(id2) + ' ' + str(id))\n",
    "                pairs += 1\n",
    "                break\n",
    "                    \n",
    "    except:\n",
    "        print(id,str(traceback.format_exc())) \n",
    "        \n",
    "features = {}\n",
    "print(pairs)\n",
    "for i,id in enumerate(batch_ids):\n",
    "    if len(batch_ids) == 0:\n",
    "        continue\n",
    "    try:\n",
    "        image = load_image(\"../input2/data/\", id_pic[id])\n",
    "        features[id] = image[80:100:2,175:200:2,:].astype(int)\n",
    "        if len(set(features[id].sum(axis = 2).reshape(-1).tolist())) <= 1:\n",
    "            continue\n",
    "        for id2,v in features.items():\n",
    "            if id == id2 or str(id) + ' ' + str(id2) in isfound3:\n",
    "                continue\n",
    "            if abs(np.mean(v - features[id])) < 2.5 and np.std(v - features[id]) < 2.5 and np.std(features[id].sum(axis = 2)) > 5:\n",
    "                print(str(id) + ' ' + str(id2),file = f)\n",
    "#                 print(id,id2)\n",
    "                isfound3.add(str(id) + ' ' + str(id2))\n",
    "                isfound3.add(str(id2) + ' ' + str(id))\n",
    "                pairs += 1\n",
    "                break\n",
    "                    \n",
    "    except:\n",
    "        print(id,str(traceback.format_exc()))           \n",
    "                \n",
    "print(pairs)\n",
    "print(len(isfound))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8654, 65908] 1 0\n",
      "[8654, 65908] 1 0\n",
      "718\n"
     ]
    }
   ],
   "source": [
    "f = open('./same_id.csv','w+')\n",
    "bad_id = set()\n",
    "for pair in isfound3:\n",
    "    if pair in isfound:\n",
    "        array = sorted([int(x) for x in pair.split(\" \")])\n",
    "        if id_label.get(int(array[0]),-1) != id_label.get(int(array[1]),-1) and id_label.get(int(array[0]),-1) != -1 and id_label.get(int(array[1]),-1) != -1:\n",
    "            print(array,id_label.get(int(array[0]),-1),id_label.get(int(array[1]),-1))\n",
    "        else:\n",
    "            print(str(array[0]) + \" \" + str(array[1]),file = f)\n",
    "            bad_id.add(array[1])\n",
    "print(len(bad_id))                \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
